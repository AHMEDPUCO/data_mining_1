# Backfill QuickBooks Online (QBO) ‚Üí Postgres con Mage


La arquitectura del proyecto se encarga de obtener datos hist√≥ricos sandbox de la API de QuickBooks para poder almacenarlos en una base de datos. 

La arquitectura de cada pipeline en el orquestador *Mage* es la siguiente:

En el loader, obtenemos la informaci√≥n de la api en formato JSON, calculamos las m√©tricas solicitadas y pasamos la informaci√≥n al exporter.

En el exporter, definimos el esquema raw de las tablas de la base de datos, definimos un template de que informaci√≥n se espera en cada columna, obtenemos el json payload y subimos a *Postgres*


## Caracter√≠sticas principales

- Segmentaci√≥n de rangos `[fecha_inicio, fecha_fin)` en: `day | week | month | quarter | year` (UTC).  
- Extracci√≥n v√≠a API QBO **OAuth2**.  
- Paginaci√≥n completa con **backoff exponencial + jitter** para evitar problemas de carga.  
- Carga **idempotente** en Postgres (`UPSERT`).  
- Emisi√≥n de m√©tricas por batch: p√°ginas, filas, insertados, actualizados, omitidos, duraci√≥n.  

---

##  Arquitectura

![Arquitectura](evidencias/Diagrama.png)

---

##  C√≥mo levantar el proyecto

1. **Clonar el repositorio**  
   - `git clone https://github.com/AHMEDPUCO/data_mining_1.git`  

2. **Configuramos el .env para poder levantar el postgres**  
   - Definimos valores para:
     * PGUSER=your_username
     * PGPASSWORD=your_password
     * PGDATABASE=your_database
     * PGADMIN_EMAIL=your_email
     * PGADMIN_PASSWORD=your_password
   - 
2. **Levantamos el contenedor**  
   - `docker compose up -d`  

3. **Ingresar a la UI de mage**  
   - En el navegador: [http://localhost:6789](http://localhost:6789)  

4. **Abrir pgAdmin**  
   - Validar conexi√≥n al Postgres.  
---

###  Gesti√≥n de secretos (prop√≥sito/rotaci√≥n/responsables)
| Nombre              | Prop√≥sito                                   | Rotaci√≥n recomendada              | Responsable     |
|---------------------|---------------------------------------------|-----------------------------------|-----------------|
| QBO_CLIENT_ID ,QBO_CLIENT_SECRET      | ID y secreto de la app QBO (OAuth2)                   | Rota seg√∫n la aplicaci√≥n         | TI   |
| QBO_REFRESH_TOKEN   | Necesario para poder generar access_token     |Rota por expiraci√≥n o por cantidad de usos |  TI   |
| QBO_REALM_ID        | Company ID de QBO                           | Est√°tica (por compa√±√≠a)           | Data Eng        |
| PG_HOST/PORT/DB     | Conexi√≥n a Postgres                         | No rota,a menos, que el DBA haga cambios          | Plataforma      |
| PG_USER/PASSWORD    | Credenciales de Postgres                    | No rota, a menos que cambie de usuario                | Plataforma      |

> En la carpeta del proyecto definimos un .env que contiene las credenciales para poder arrancar Postgres. En este caso, subimos un .env.example para que defina las variables y se pueda alzar el postgres.
---

## Pipelines
## Par√°metros
Los parametros que le vamos a pasar son los siguientes
- 'chunk_size'
- 'minor_version'
- 'page_size'
- 'fecha_inicio'
- 'fecha_fin'
Estos son ingresados desde el trigger; sin embargo, se ponen valores por default para evitar posibles errores de ejecuci√≥n.


##Estructura
### `qb_customers_backfill`
- `qb_customer_loader`  
- `qb_customer_exporter`  
  

### `qb_invoices_backfill`
- `qb_invoices_loader`  
- `qb_invoices_exporter`  

### `qb_items_backfill`
- `qb_items_loader`  
- `qb_items_exporter`  

---

## SEGMENTACION CHUNKING
Una vez indicada la fecha de inicio y fecha de fin, creamos varios chunks que van a procesar la informaci√≥n en cierto rango de fecha. Este rango lo definimos nosotros en los par√°metros. Esto lo hacemos con el fin de evitar timeouts o respuestas demasiados grandes.

## PAGINACI√ìN
La ap de quicbooks limita la cantidad de resultados por request. 
Por lo tanto, implementamos paginaci√≥n en cada chunk, donde vamos a indicar desde que pagina se empieza y el m√°ximo de resultados por pagina. 
Utilizamos una funci√≥n para que realice esto varias veces hasta agotar todos los datos.

## LIMITES
QuickBooks tiene l√≠mites importantes:
- **Rate limit**: Error 429 (Too Many Requests)
- **Errores 5xx**: Errores internos de QuickBooks
- **Timeouts**: Demasiada informaci√≥n en un solo request

**Manejo en el c√≥digo:**

- Reintentos autom√°ticos con backoff exponencial + jitter (`sleep` creciente con random).
- `Retry-After` si el header lo indica.
- M√°ximo de 6 reintentos por request.



##  Triggers One-Time

- **Tipo**: Once 
- **Parametros obligatorias**:  
  - `fecha_inicio`: ISO UTC, ej. `2025-01-01T00:00:00Z`  
  - `fecha_fin`: ISO UTC, ej. `2025-09-11T00:00:00Z`
  - 'chunk_size': default(7)
  - 'minor_version' :default(75)
  - 'page_size': default(1000)

- **Pol√≠tica post-ejecuci√≥n**: al finalizar, deshabilitar o eliminar el trigger para evitar reejecuciones accidentales.

üì∏ Evidencia del trigger de un pipeline:  
<img width="1838" height="928" src="evidencias/qb_invoices_pipeline_trigger.png" />  


---

##  Esquema RAW
El esquema RAW lo definimos en el exporter
## **Ejemplo de esquema**:
```sql
CREATE SCHEMA IF NOT EXISTS raw;
CREATE TABLE IF NOT EXISTS raw.qb_items (
  id  text PRIMARY KEY,
  payload jsonb NOT NULL,
  ingested_at_utc timestamptz NOT NULL DEFAULT now(),
  extract_window_start_utc timestamptz NOT NULL,
  extract_window_end_utc   timestamptz NOT NULL,
  page_number int,
  page_size   int,
  request_payload jsonb
);
CREATE INDEX IF NOT EXISTS idx_qb_items_ingested_at ON raw.qb_items (ingested_at_utc);
CREATE INDEX IF NOT EXISTS idx_qb_items_win_start   ON raw.qb_items (extract_window_start_utc);
CREATE INDEX IF NOT EXISTS idx_qb_items_win_end     ON raw.qb_items (extract_window_end_utc);
```

### Idempotencia

Para asegurar que los datos puedan cargarse m√∫ltiples veces **sin causar duplicados ni inconsistencias**, se aplican dos mecanismos principales:

---

####  1. Uso de UPSERT (Insert or Update)

- Utilizamos sentencias tipo `INSERT ... ON CONFLICT DO UPDATE` (tambi√©n conocidas como *upserts*).
- Esto permite que:
  - Si el registro **ya existe**, se actualiza.
  - Si el registro **no existe**, se inserta.
- Beneficios:
  - Evita insertar duplicados.
  - Garantiza que los datos m√°s recientes reemplacen a los anteriores de forma segura.
  - No es necesario borrar datos previamente cargados.

---

####  2. Funci√≥n `deduplicate` en el exporter

- Antes de enviar los datos a la base de datos, se aplica una funci√≥n de deduplicaci√≥n.
- Esta funci√≥n limpia los registros duplicados **dentro del mismo batch**.
- Beneficios:
  - Evita m√∫ltiples intentos de insertar el mismo registro.
  - Reduce la cantidad de operaciones innecesarias sobre la base de datos (mejor rendimiento).
  - Mejora la consistencia de los datos procesados.

---

##  Validaciones y Volumetr√≠a

# Validaciones y Volumetr√≠a ‚Äì QBO Backfill
##  ¬øCu√°ndo se ejecutan las validaciones?

Las validaciones se corren **autom√°ticamente al ejecutar el pipeline** desde Mage. Para analizarlas:

1. Ejecuta el trigger desde la UI o v√≠a API.
2. Revisa los logs generados por:
   - El bloque **loader** (extracci√≥n y particionamiento por chunks)
   - El bloque **exporter** (env√≠o de datos a base de datos)

---

##  Loader ‚Äì Logs

Durante la ejecuci√≥n del `loader`, se imprime informaci√≥n sobre cada p√°gina procesada, incluyendo:
P√°gina 1: 7 filas en 0.48s (start=1)
Chunk 32: 2025-08-06T00:00:00+00:00 ‚Üí 2025-08-12T23:59:59+00:00

###  ¬øQu√© significa cada parte?

- `P√°gina 1`: n√∫mero de p√°gina devuelta por la fuente de datos.
- `7 filas`: cantidad de registros devueltos en esa p√°gina.
- `start=1`: posici√≥n inicial del chunk en la paginaci√≥n total.
- `0.48s`: tiempo que tom√≥ procesar esa p√°gina.
- `Chunk 32`: n√∫mero de chunk que cubre el rango de fechas siguiente.
- `2025-08-06 ‚Üí 2025-08-12`: intervalo temporal cubierto por ese chunk.

Esta informaci√≥n permite validar:
- Que **todas las p√°ginas** est√°n siendo procesadas.
- Que no hay **saltos o superposiciones** en los chunks de fechas.
- Cu√°nto **demora cada carga parcial**.

---

##  Exporter ‚Äì Logs y su interpretaci√≥n

En el bloque `exporter`, se imprimen logs detallando c√≥mo se insertan los datos en la base de datos:

Batch INVOICES: 10 (inserted=10, updated=0, skipped=0)
Batch INVOICES: 10 (inserted=10, updated=0, skipped=0)
Batch INVOICES: 3 (inserted=3, updated=0, skipped=0)
Carga INVOICES: 23 filas en 0.12s (inserted=23, updated=0, skipped=0)

###  ¬øC√≥mo interpretar estos logs?

- Cada l√≠nea `Batch INVOICES` representa un batch procesado.
- `inserted`: filas nuevas agregadas.
- `updated`: filas que ya exist√≠an y fueron modificadas.
- `skipped`: filas ignoradas (por duplicados o validaciones).
- L√≠nea final `Carga`: resumen total del proceso.

Estos logs ayudan a validar que:
- La carga fue **exitosa**.
- Los datos fueron correctamente **insertados o actualizados**.
- No se est√°n **omitiendo** registros importantes.
- Aeguran la volumetr√≠a y la idempotencia(al volver a correr si en skipped sale el numero total de filas , significa que no hubo duplicaciones)

---

## Verificaci√≥n de Volumetr√≠a

La volumetr√≠a se puede verificar en tres niveles:

1. **Loader:** verifica cu√°ntas filas se reciben por p√°gina o chunk.
2. **Exporter:** verifica cu√°ntas filas se insertan, actualizan o se omiten.
3. **Base de datos:** usa una consulta SQL (`SELECT COUNT(*) ...`) para validar el n√∫mero total de filas realmente insertadas en la tabla.

Al comparar los totales entre estas tres etapas, puedes confirmar si la carga fue completa y correcta.


---

##  Troubleshooting

###  Autenticaci√≥n
Para no tener problemas de autenticaci√≥n, debemos asegurarnos de tener configurados correctamente los secretos.
En el caso de tener errores relacionados al acces_token, cambiar el refresh token.


###  Paginaci√≥n
Para la paginaci√≥n, usamos start position y maxresults para comprobar que revisamos toda la pagina. Considerar que los chunks siempre van a iniciar en 1 para no perder informaci√≥n. Si no empezamos en 1, podr√≠amos perder datos.


###  L√≠mites
Respecto a los limites, definimos estrategias como el backoff + jitter y un numero m√°ximo de intentos. Adem√°s, tomamos en cuenta tambi√©n el valor devuelto por los headers como alternativa al backoff.

- Si se recibe error 429, se reintenta hasta 6 veces con backoff.
- Reduzca la granularidad (`days_per_chunk`) si hay muchos errores por l√≠mite.

###  Timezones
Todas las fechas est√°n en UTC, Los timestamps de QBO incluyen zona horaria.


###  Almacenamiento
Los datos se insertan en las bases de datos respectivas e implementamos upsert y funci√≥n de deduplicaci√≥n para asegurar la integridad de los datos ingresaods. Si cambiamos el payload, se actualiza el registro.

###  Permisos
- El token debe tener acceso a la entidad consultada (`Item`, `Invoice`, etc.).
- El usuario de DB debe tener permisos de escritura.

## Checklist de Aceptaci√≥n

- [x] Mage y Postgres se comunican por nombre de servicio.  
- [x] Todos los secretos est√°n en Mage (no en el repo).  
- [x] Pipelines aceptan `fecha_inicio` y `fecha_fin` en UTC.  
- [x] Trigger one-time configurado, ejecutado y luego deshabilitado.  
- [x] Esquema RAW creado con payload completo y metadatos.  
- [x] Idempotencia verificada (`ON CONFLICT`).  
- [x] Paginaci√≥n y rate limits manejados y documentados.  
- [x] Validaciones de volumetr√≠a registradas como evidencia.  
- [x] Runbook de reintentos y reanudaci√≥n disponible.  

---

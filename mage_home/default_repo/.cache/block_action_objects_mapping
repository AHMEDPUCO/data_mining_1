{"block_file": {"data_exporters/qb_customer_exporter.py:data_exporter:python:qb customer exporter": {"content": "# export_customers_to_postgres.py\nimport json, time\nfrom datetime import datetime, timezone\nimport psycopg2\nfrom psycopg2.extras import execute_values  # \ud83d\udc48 antes usabas execute_batch\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\nCREATE TABLE IF NOT EXISTS raw.qb_customers (\n  id  text PRIMARY KEY,\n  payload jsonb NOT NULL,\n  ingested_at_utc timestamptz NOT NULL DEFAULT now(),\n  extract_window_start_utc timestamptz NOT NULL,\n  extract_window_end_utc   timestamptz NOT NULL,\n  page_number int,\n  page_size   int,\n  request_payload jsonb\n);\nCREATE INDEX IF NOT EXISTS idx_qb_customers_ingested_at ON raw.qb_customers (ingested_at_utc);\nCREATE INDEX IF NOT EXISTS idx_qb_customers_win_start   ON raw.qb_customers (extract_window_start_utc);\nCREATE INDEX IF NOT EXISTS idx_qb_customers_win_end     ON raw.qb_customers (extract_window_end_utc);\n\"\"\"\n\n#  UPSERT con VALUES %s + RETURNING para contar insertados/actualizados\nUPSERT = \"\"\"\nINSERT INTO raw.qb_customers (\n  id, payload, extract_window_start_utc, extract_window_end_utc,\n  page_number, page_size, request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE\nSET payload = EXCLUDED.payload,\n    ingested_at_utc = now(),\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc   = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size   = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload\nWHERE raw.qb_customers.payload IS DISTINCT FROM EXCLUDED.payload\nRETURNING\n  (xmax = 0)  AS inserted,\n  (xmax <> 0) AS updated;\n\"\"\"\n\n#XMAX es una funci\u00f3n propia de postgresql que nos ayuda a  verificar si una fila fue insertada o actualizada\ndef _conn():\n    return psycopg2.connect(\n        host=get_secret_value('DB_HOST') or 'postgres',\n        port=int(get_secret_value('DB_PORT') or 5432),\n        dbname=get_secret_value('DB_NAME') or 'qbo_dw',\n        user=get_secret_value('DB_USER'),\n        password=get_secret_value('DB_PASSWORD'),\n    )\n\ndef _utc_iso_now():\n    return datetime.now(timezone.utc).isoformat()\n\ndef _iterate_batches(lst, batch_size=1000):\n    for i in range(0, len(lst), batch_size):\n        yield lst[i:i+batch_size]\n\n@data_exporter\ndef export_invoices_to_postgres(data, *args, **kwargs):\n    if not isinstance(data, dict):\n        raise ValueError(\"Se esperaba un dict como output del loader.\")\n    rows = data.get('data') or []\n    audit = data.get('audit') or []\n    if not rows:\n        print(\"No hay invoices para insertar.\")\n        return {\"processed\": 0}\n\n    # Deduplicaci\u00f3n simple por Id\n    seen = {}\n    for r in rows:\n        if isinstance(r, dict) and r.get(\"Id\"):\n            seen[r[\"Id\"]] = r\n    rows = list(seen.values())\n    if not rows:\n        print(\"Sin filas v\u00e1lidas con Id para invoices.\")\n        return {\"processed\": 0}\n\n    win = audit[0] if audit else {\n        \"window_start_utc\": \"2001-01-01T00:00:00Z\",\n        \"window_end_utc\": \"2025-09-11T23:59:59Z\",\n        \"pages\": None, \"page_size\": None,\n    }\n\n    batch_size = int(kwargs.get(\"db_batch_size\") or 10)\n    t0 = time.time()\n    total_inserted = total_updated = total_skipped = 0\n\n\n    #definici\u00f3n de formato de columnas(que datos se espera recibir en cada columna)\n    with _conn() as conn:\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n            for b in _iterate_batches(rows, batch_size):\n                vals = []\n                for row in b:\n                    win_start = row.get(\"_win_start_utc\", win[\"window_start_utc\"])\n                    win_end   = row.get(\"_win_end_utc\",   win[\"window_end_utc\"])\n                    vals.append((\n                        row[\"Id\"],\n                        json.dumps(row),\n                        win_start,\n                        win_end,\n                        win.get(\"pages\"),\n                        win.get(\"page_size\"),\n                        json.dumps({\n                            \"env\": \"sandbox\",\n                            \"source\": \"mage\",\n                            \"minor_version\": data.get(\"minor_version\"),\n                            \"generated_at_utc\": data.get(\"generated_at_utc\"),\n        }),\n    ))\n\n\n                template = \"(\" \\\n                           \"%s, \" \\\n                           \"%s::jsonb, \" \\\n                           \"%s::timestamptz, %s::timestamptz, \" \\\n                           \"%s, %s, \" \\\n                           \"%s::jsonb\" \\\n                           \")\"\n\n                ret = execute_values(\n                    cur,\n                    UPSERT,\n                    vals,\n                    template=template,\n                    page_size=200,\n                    fetch=True,\n                )\n\n                inserted = sum(1 for r in ret if r[0] is True)\n                updated  = sum(1 for r in ret if r[1] is True)\n                skipped  = len(b) - (inserted + updated)\n\n                total_inserted += inserted\n                total_updated  += updated\n                total_skipped  += skipped\n\n                print(f\"Batch INVOICES: {len(b)} (inserted={inserted}, updated={updated}, skipped={skipped})\")\n\n        conn.commit()\n\n    elapsed = round(time.time() - t0, 2)\n    print(f\"Carga INVOICES: {len(rows)} filas en {elapsed}s \"\n          f\"(inserted={total_inserted}, updated={total_updated}, skipped={total_skipped})\")\n\n    return {\n        \"processed\": len(rows),\n        \"inserted\": total_inserted,\n        \"updated\": total_updated,\n        \"skipped\": total_skipped,\n        \"elapsed_sec\": elapsed,\n        \"finished_at_utc\": _utc_iso_now(),\n    }", "file_path": "data_exporters/qb_customer_exporter.py", "language": "python", "type": "data_exporter", "uuid": "qb_customer_exporter"}, "data_exporters/qb_invoices_exporter.py:data_exporter:python:qb invoices exporter": {"content": "import json, time\nfrom datetime import datetime, timezone\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\nCREATE TABLE IF NOT EXISTS raw.qb_invoices (\n  id  text PRIMARY KEY,\n  payload jsonb NOT NULL,\n  ingested_at_utc timestamptz NOT NULL DEFAULT now(),\n  extract_window_start_utc timestamptz NOT NULL,\n  extract_window_end_utc   timestamptz NOT NULL,\n  page_number int,\n  page_size   int,\n  request_payload jsonb\n);\nCREATE INDEX IF NOT EXISTS idx_qb_invoices_ingested_at ON raw.qb_invoices (ingested_at_utc);\nCREATE INDEX IF NOT EXISTS idx_qb_invoices_win_start   ON raw.qb_invoices (extract_window_start_utc);\nCREATE INDEX IF NOT EXISTS idx_qb_invoices_win_end     ON raw.qb_invoices (extract_window_end_utc);\n\"\"\"\n\nUPSERT = \"\"\"\nINSERT INTO raw.qb_invoices (\n  id, payload, extract_window_start_utc, extract_window_end_utc,\n  page_number, page_size, request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE\nSET payload = EXCLUDED.payload,\n    ingested_at_utc = now(),\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc   = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size   = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload\nWHERE raw.qb_invoices.payload IS DISTINCT FROM EXCLUDED.payload\nRETURNING\n  (xmax = 0)  AS inserted,\n  (xmax <> 0) AS updated;\n\"\"\"\n#XMAX es una funci\u00f3n propia de postgresql que nos ayuda a  verificar si una fila fue insertada o actualizada\ndef _conn():\n    return psycopg2.connect(\n        host=get_secret_value('DB_HOST') or 'postgres',\n        port=int(get_secret_value('DB_PORT') or 5432),\n        dbname=get_secret_value('DB_NAME') or 'qbo_dw',\n        user=get_secret_value('DB_USER'),\n        password=get_secret_value('DB_PASSWORD'),\n    )\n\ndef _utc_iso_now():\n    return datetime.now(timezone.utc).isoformat()\n\ndef _iterate_batches(lst, batch_size=1000):\n    for i in range(0, len(lst), batch_size):\n        yield lst[i:i+batch_size]\n\n@data_exporter\ndef export_invoices_to_postgres(data, *args, **kwargs):\n    if not isinstance(data, dict):\n        raise ValueError(\"Se esperaba un dict como output del loader.\")\n    rows = data.get('data') or []\n    audit = data.get('audit') or []\n    if not rows:\n        print(\"No hay invoices para insertar.\")\n        return {\"processed\": 0}\n\n    # Deduplicaci\u00f3n simple por Id\n    seen = {}\n    for r in rows:\n        if isinstance(r, dict) and r.get(\"Id\"):\n            seen[r[\"Id\"]] = r\n    rows = list(seen.values())\n    if not rows:\n        print(\"Sin filas v\u00e1lidas con Id para invoices.\")\n        return {\"processed\": 0}\n\n    win = audit[0] if audit else {\n        \"window_start_utc\": \"2001-01-01T00:00:00Z\",\n        \"window_end_utc\": \"2025-09-11T23:59:59Z\",\n        \"pages\": None, \"page_size\": None,\n    }\n\n    batch_size = int(kwargs.get(\"db_batch_size\") or 10)\n    t0 = time.time()\n    total_inserted = total_updated = total_skipped = 0\n\n\n    #definici\u00f3n de formato de columnas(que datos se espera recibir en cada columna)\n    with _conn() as conn:\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n            for b in _iterate_batches(rows, batch_size):\n                vals = []\n                for row in b:\n                    win_start = row.get(\"_win_start_utc\", win[\"window_start_utc\"])\n                    win_end   = row.get(\"_win_end_utc\",   win[\"window_end_utc\"])\n                    vals.append((\n                        row[\"Id\"],\n                        json.dumps(row),\n                        win_start,\n                        win_end,\n                        win.get(\"pages\"),\n                        win.get(\"page_size\"),\n                        json.dumps({\n                            \"env\": \"sandbox\",\n                            \"source\": \"mage\",\n                            \"minor_version\": data.get(\"minor_version\"),\n                            \"generated_at_utc\": data.get(\"generated_at_utc\"),\n        }),\n    ))\n\n\n                template = \"(\" \\\n                           \"%s, \" \\\n                           \"%s::jsonb, \" \\\n                           \"%s::timestamptz, %s::timestamptz, \" \\\n                           \"%s, %s, \" \\\n                           \"%s::jsonb\" \\\n                           \")\"\n\n                ret = execute_values(\n                    cur,\n                    UPSERT,\n                    vals,\n                    template=template,\n                    page_size=200,\n                    fetch=True,\n                )\n\n                inserted = sum(1 for r in ret if r[0] is True)\n                updated  = sum(1 for r in ret if r[1] is True)\n                skipped  = len(b) - (inserted + updated)\n\n                total_inserted += inserted\n                total_updated  += updated\n                total_skipped  += skipped\n\n                print(f\"Batch INVOICES: {len(b)} (inserted={inserted}, updated={updated}, skipped={skipped})\")\n\n        conn.commit()\n\n    elapsed = round(time.time() - t0, 2)\n    print(f\"Carga INVOICES: {len(rows)} filas en {elapsed}s \"\n          f\"(inserted={total_inserted}, updated={total_updated}, skipped={total_skipped})\")\n\n    return {\n        \"processed\": len(rows),\n        \"inserted\": total_inserted,\n        \"updated\": total_updated,\n        \"skipped\": total_skipped,\n        \"elapsed_sec\": elapsed,\n        \"finished_at_utc\": _utc_iso_now(),\n    }", "file_path": "data_exporters/qb_invoices_exporter.py", "language": "python", "type": "data_exporter", "uuid": "qb_invoices_exporter"}, "data_exporters/qb_items_exporter.py:data_exporter:python:qb items exporter": {"content": "import json, time\nfrom datetime import datetime, timezone\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\nCREATE TABLE IF NOT EXISTS raw.qb_items (\n  id  text PRIMARY KEY,\n  payload jsonb NOT NULL,\n  ingested_at_utc timestamptz NOT NULL DEFAULT now(),\n  extract_window_start_utc timestamptz NOT NULL,\n  extract_window_end_utc   timestamptz NOT NULL,\n  page_number int,\n  page_size   int,\n  request_payload jsonb\n);\nCREATE INDEX IF NOT EXISTS idx_qb_items_ingested_at ON raw.qb_items (ingested_at_utc);\nCREATE INDEX IF NOT EXISTS idx_qb_items_win_start   ON raw.qb_items (extract_window_start_utc);\nCREATE INDEX IF NOT EXISTS idx_qb_items_win_end     ON raw.qb_items (extract_window_end_utc);\n\"\"\"\n\nUPSERT = \"\"\"\nINSERT INTO raw.qb_items (\n  id, payload, extract_window_start_utc, extract_window_end_utc,\n  page_number, page_size, request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE\nSET payload = EXCLUDED.payload,\n    ingested_at_utc = now(),\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc   = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size   = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload\nWHERE raw.qb_items.payload IS DISTINCT FROM EXCLUDED.payload\nRETURNING\n  (xmax = 0)  AS inserted,\n  (xmax <> 0) AS updated;\n\"\"\"\n#XMAX es una funci\u00f3n propia de postgresql que nos ayuda a  verificar si una fila fue insertada o actualizada\ndef _conn():\n    return psycopg2.connect(\n        host=get_secret_value('DB_HOST') or 'postgres',\n        port=int(get_secret_value('DB_PORT') or 5432),\n        dbname=get_secret_value('DB_NAME') or 'qbo_dw',\n        user=get_secret_value('DB_USER'),\n        password=get_secret_value('DB_PASSWORD'),\n    )\n\ndef _utc_iso_now():\n    return datetime.now(timezone.utc).isoformat()\n\ndef _iterate_batches(lst, batch_size=1000):\n    for i in range(0, len(lst), batch_size):\n        yield lst[i:i+batch_size]\n\n@data_exporter\ndef export_invoices_to_postgres(data, *args, **kwargs):\n    if not isinstance(data, dict):\n        raise ValueError(\"Se esperaba un dict como output del loader.\")\n    rows = data.get('data') or []\n    audit = data.get('audit') or []\n    if not rows:\n        print(\"No hay invoices para insertar.\")\n        return {\"processed\": 0}\n\n    # Deduplicaci\u00f3n simple por Id\n    seen = {}\n    for r in rows:\n        if isinstance(r, dict) and r.get(\"Id\"):\n            seen[r[\"Id\"]] = r\n    rows = list(seen.values())\n    if not rows:\n        print(\"Sin filas v\u00e1lidas con Id para invoices.\")\n        return {\"processed\": 0}\n\n    win = audit[0] if audit else {\n        \"window_start_utc\": \"2001-01-01T00:00:00Z\",\n        \"window_end_utc\": \"2025-09-11T23:59:59Z\",\n        \"pages\": None, \"page_size\": None,\n    }\n\n    batch_size = int(kwargs.get(\"db_batch_size\") or 10)\n    t0 = time.time()\n    total_inserted = total_updated = total_skipped = 0\n\n\n    #definici\u00f3n de formato de columnas(que datos se espera recibir en cada columna)\n    with _conn() as conn:\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n            for b in _iterate_batches(rows, batch_size):\n                vals = []\n                for row in b:\n                    win_start = row.get(\"_win_start_utc\", win[\"window_start_utc\"])\n                    win_end   = row.get(\"_win_end_utc\",   win[\"window_end_utc\"])\n                    vals.append((\n                        row[\"Id\"],\n                        json.dumps(row),\n                        win_start,\n                        win_end,\n                        win.get(\"pages\"),\n                        win.get(\"page_size\"),\n                        json.dumps({\n                            \"env\": \"sandbox\",\n                            \"source\": \"mage\",\n                            \"minor_version\": data.get(\"minor_version\"),\n                            \"generated_at_utc\": data.get(\"generated_at_utc\"),\n        }),\n    ))\n\n\n                template = \"(\" \\\n                           \"%s, \" \\\n                           \"%s::jsonb, \" \\\n                           \"%s::timestamptz, %s::timestamptz, \" \\\n                           \"%s, %s, \" \\\n                           \"%s::jsonb\" \\\n                           \")\"\n\n                ret = execute_values(\n                    cur,\n                    UPSERT,\n                    vals,\n                    template=template,\n                    page_size=200,\n                    fetch=True,\n                )\n\n                inserted = sum(1 for r in ret if r[0] is True)\n                updated  = sum(1 for r in ret if r[1] is True)\n                skipped  = len(b) - (inserted + updated)\n\n                total_inserted += inserted\n                total_updated  += updated\n                total_skipped  += skipped\n\n                print(f\"Batch INVOICES: {len(b)} (inserted={inserted}, updated={updated}, skipped={skipped})\")\n\n        conn.commit()\n\n    elapsed = round(time.time() - t0, 2)\n    print(f\"Carga INVOICES: {len(rows)} filas en {elapsed}s \"\n          f\"(inserted={total_inserted}, updated={total_updated}, skipped={total_skipped})\")\n\n    return {\n        \"processed\": len(rows),\n        \"inserted\": total_inserted,\n        \"updated\": total_updated,\n        \"skipped\": total_skipped,\n        \"elapsed_sec\": elapsed,\n        \"finished_at_utc\": _utc_iso_now(),\n    }\n", "file_path": "data_exporters/qb_items_exporter.py", "language": "python", "type": "data_exporter", "uuid": "qb_items_exporter"}, "data_loaders/qb_customer_loader.py:data_loader:python:qb customer loader": {"content": "import requests\nimport base64\nimport time\nimport random\nfrom datetime import datetime, timedelta, timezone\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\ntry:\n    from mage_ai.data_preparation.shared.secrets import set_secret_value\nexcept Exception:\n    set_secret_value = None\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n# --------------------------\n# Utilidades\n# --------------------------\ndef _now_utc():\n    return datetime.now(timezone.utc)\n\n\ndef _parse_iso_utc(s: str) -> datetime:\n    \"\"\"\n    Admite 'YYYY-MM-DD' (asume 00:00:00Z)\n    \"\"\"\n    if 'T' not in s:\n        s = s.strip() + \"T00:00:00Z\"\n    return datetime.fromisoformat(s.replace('Z', '+00:00')).astimezone(timezone.utc)\n\n\ndef _date_range_chunks(start_utc_day: datetime, end_utc_day: datetime, days_per_chunk: int = 1):\n    \"\"\"\nGenerador rango de ventanas chunk\n    \"\"\"\n    if start_utc_day.tzinfo is None or end_utc_day.tzinfo is None:\n        raise ValueError(\"Las fechas deben ser timezone-aware (UTC).\")\n    current_day = start_utc_day\n    step_day = timedelta(days=days_per_chunk)\n    while current_day <= end_utc_day:\n        chunk_end = min(current_day + step_day - timedelta(seconds=1), end_utc_day)\n        yield current_day, chunk_end\n        current_day = current_day + step_day\n\n\n# --------------------------\n# OAuth2: refresh token\n# --------------------------\ndef refresh_access_token():\n\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    if not all([client_id, client_secret, refresh_token]):\n        raise ValueError(\"Faltan secrets: qb_client_id, qb_client_secret o qb_refresh_token\")\n\n    # Codificar client_id:client_secret en base64\n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode('utf-8')).decode('utf-8')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Authorization\": f\"Basic {encoded_credentials}\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, timeout=30)\n    try:\n        resp.raise_for_status()\n    except Exception as e:\n        raise RuntimeError(f\"Error al refrescar token: {resp.status_code} {resp.text}\") from e\n\n    tokens = resp.json()\n    new_access_token = tokens.get('access_token')\n    new_refresh_token = tokens.get('refresh_token')\n\n    if not new_access_token:\n        raise RuntimeError(\"Respuesta inv\u00e1lida: no se recibi\u00f3 access_token\")\n\n    # Rotaci\u00f3n de refresh_token\n    if new_refresh_token and new_refresh_token != refresh_token and set_secret_value:\n        try:\n            set_secret_value('qb_refresh_token', new_refresh_token)\n            print(\"Refresh token rotado en Mage Secrets.\")\n        except Exception:\n            print(\"No se pudo persistir el nuevo refresh_token en Mage Secrets. R\u00f3talo manualmente.\")\n\n    print(\"Access token refrescado exitosamente.\")\n    return new_access_token\n\n\n# --------------------------\n#Paginaci\u00f3n con l\u00f3gica de backoff implementada\n# --------------------------\ndef _qbo_query_page(session: requests.Session, base_url: str, realm_id: str,\n                    access_token: str, minor_version: int, qbo_sql: str,\n                    startposition: int, maxresults: int):\n    \"\"\"\n    Ejecuta UNA p\u00e1gina de la QBO SQL Query con manejo de rate limits (429) y 5xx (backoff + jitter).\n    \"\"\"\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere url base y minor_version\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain',\n    }\n    params = {\n        'query': f\"{qbo_sql} startposition {startposition} maxresults {maxresults}\",\n        'minorversion': minor_version,\n    }\n\n    max_retries = 6  #Numero m\u00e1ximo de reintentos\n    base_sleep = 1.5  # segundos para backofff exponencial\n\n    for attempt in range(1, max_retries + 1):\n        resp = session.get(url, headers=headers, params=params, timeout=60)\n        if resp.status_code == 429 or 500 <= resp.status_code < 600:  # 429(rate limit) , 5xx(Error del servidor)            \n            retry_after = resp.headers.get(\"Retry-After\") \n            if retry_after:  #Usamos valor dado por el retry after\n                sleep_s = float(retry_after)\n            else:\n                sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0, 0.3)  #Backoof exponencial con jitter aleatorio para evitar demasiados reintentos en el mismo tiempo \n            print(f\"Retry {attempt}/{max_retries} por {resp.status_code}. Durmiendo {sleep_s:.1f}s\")\n            time.sleep(sleep_s)\n            continue\n        try:\n            resp.raise_for_status()\n            data = resp.json() #Se devuelve la informacion en forma de diccionario\n            return data\n        except Exception as e:\n            raise RuntimeError(f\"Error QBO query (HTTP {resp.status_code}): {resp.text}\") from e \n\n    raise RuntimeError(\"Se agotaron los reintentos contra QBO.\")\n\n\n# --------------------------\n# Iterar todas las p\u00e1ginas\n# --------------------------\ndef _fetch_qb_data(session: requests.Session, realm_id: str, access_token: str,\n                   qbo_sql: str, base_url: str, minor_version: int, page_size: int = 1000):\n    \"\"\"\n    Itera todas las p\u00e1ginas hasta agotar resultados.\n    Devuelve data y m\u00e9tricas\n    \"\"\"\n    startposition = 1\n    total_rows = 0\n    page_number = 0\n    items = []\n\n    while True:\n        page_number += 1\n        t0 = time.time()\n        data = _qbo_query_page(\n            session, base_url, realm_id, access_token, minor_version,\n            qbo_sql, startposition, page_size\n        )\n        qr = data.get(\"QueryResponse\", {})\n        rows = qr.get(\"Customer\", [])\n        items.extend(rows)\n\n        elapsed = time.time() - t0\n        print(f\"P\u00e1gina {page_number}: {len(rows)} filas en {elapsed:.2f}s (start={startposition})\")\n\n        total_rows += len(rows)\n        # Si trae menos que page_size, no hay m\u00e1s p\u00e1ginas\n        if len(rows) < page_size:\n            break\n\n        startposition += page_size\n        #Metricas solicitadas\n    metrics = {\n        \"pages\": page_number,\n        \"page_size\": page_size,\n        \"rows_total\": total_rows,\n    }\n    return items, metrics\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Parametros a pasar:    \n      - fecha_inicio (\"2025-08-01T00:00:00Z\")\n      - fecha_fin    (\"2025-08-28T23:59:59Z\")\n      - days_per_chunk: int ( 1)\n      - page_size: int (1000)\n      - minor_version: int (75)\n    \"\"\"\n\n    realm_id = get_secret_value('qb_realm_id')\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n\n    if not all([realm_id, client_id, client_secret]):\n        raise ValueError(\"Faltan secrets: Realm_id, qb_client_id, qb_client_secret\")\n\n    access_token = refresh_access_token()\n\n    # Configuraci\u00f3n fechas\n    fecha_inicio = kwargs.get('fecha_inicio',\"2025-01-01\")\n    fecha_fin = kwargs.get('fecha_fin',\"2025-09-10\")\n    \n    \n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debe proporcionar 'fecha_inicio' y 'fecha_fin'.\")\n\n    start_utc = _parse_iso_utc(fecha_inicio)\n    end_utc = _parse_iso_utc(fecha_fin)\n\n    minor_version = int(kwargs.get('minor_version',25))\n    page_size = int(kwargs.get('page_size', 1000))\n    days_per_chunk = int(kwargs.get('days_per_chunk', 7))\n    \n    #Formaci\u00f3n del url\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    session = requests.Session()\n    all_rows = []\n    audit = []\n    chunk_idx = 0\n\n\n    for win_start, win_end in _date_range_chunks(start_utc, end_utc, days_per_chunk=days_per_chunk):\n        chunk_idx += 1\n        print(f\"Chunk {chunk_idx}: {win_start.isoformat()} \u2192 {win_end.isoformat()}\")\n        fecha_ini_str = win_start.strftime('%Y-%m-%d')\n        fecha_fin_str = win_end.strftime('%Y-%m-%d')\n\n        #SE CONSTRUYE CONSULTA PARA CADA RANGO DE FECHAS DEPENDIENDO DEL CHUNK\n        qbo_sql = f\"\"\"\n            select * from Customer\n            where Metadata.CreateTime >= '{fecha_ini_str}' and Metadata.CreateTime <= '{fecha_fin_str}'\n            order by Metadata.CreateTime asc\n        \"\"\".strip()\n\n      \n\n        t0 = time.time()\n        rows, metrics = _fetch_qb_data(\n            session, realm_id, access_token, qbo_sql, base_url, minor_version, \n           \n            page_size=page_size\n        )\n        elapsed = time.time() - t0\n        for r in rows:\n            r[\"_win_start_utc\"] = win_start.isoformat()\n            r[\"_win_end_utc\"]   = win_end.isoformat()\n        all_rows.extend(rows)\n        \n        audit.append({\n            \"chunk\": chunk_idx,\n            \"window_start_utc\": win_start.isoformat(),\n            \"window_end_utc\": win_end.isoformat(),\n            \"pages\": metrics[\"pages\"],\n            \"page_size\": metrics[\"page_size\"],\n            \"rows\": metrics[\"rows_total\"],\n            \"duration_sec\": round(elapsed, 2),\n        })\n\n        \n\n    result = {\n        \"minor_version\": minor_version,\n        \"count\": len(all_rows),\n        \"data\": all_rows,\n        \"audit\": audit,\n        \"generated_at_utc\": _now_utc().isoformat(),\n    }\n\n    return result\n\n\n\n\n# --------------------------\n# Test del block\n# --------------------------\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output es None'\n    assert isinstance(output, dict), 'Output debe ser dict'\n    assert 'data' in output and isinstance(output['data'], list), 'Output[\"data\"] debe ser lista'\n    assert 'audit' in output and isinstance(output['audit'], list), 'Output[\"audit\"] debe ser lista'\n    if output.get('count', 0) > 0:\n        sample = output['data'][0]\n        assert isinstance(sample, dict), 'Cada fila debe ser un dict (JSON)'\n\n", "file_path": "data_loaders/qb_customer_loader.py", "language": "python", "type": "data_loader", "uuid": "qb_customer_loader"}, "data_loaders/qb_invoices_loader.py:data_loader:python:qb invoices loader": {"content": "import requests\nimport base64\nimport time\nimport random\nfrom datetime import datetime, timedelta, timezone\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\ntry:\n    from mage_ai.data_preparation.shared.secrets import set_secret_value\nexcept Exception:\n    set_secret_value = None\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n# --------------------------\n# Utilidades\n# --------------------------\ndef _now_utc():\n    return datetime.now(timezone.utc)\n\n\ndef _parse_iso_utc(s: str) -> datetime:\n    \"\"\"\n    Admite 'YYYY-MM-DD' (asume 00:00:00Z)\n    \"\"\"\n    if 'T' not in s:\n        s = s.strip() + \"T00:00:00Z\"\n    return datetime.fromisoformat(s.replace('Z', '+00:00')).astimezone(timezone.utc)\n\n\ndef _date_range_chunks(start_utc_day: datetime, end_utc_day: datetime, days_per_chunk: int = 1):\n    \"\"\"\nGenerador rango de ventanas chunk\n    \"\"\"\n    if start_utc_day.tzinfo is None or end_utc_day.tzinfo is None:\n        raise ValueError(\"Las fechas deben ser timezone-aware (UTC).\")\n    current_day = start_utc_day\n    step_day = timedelta(days=days_per_chunk)\n    while current_day <= end_utc_day:\n        chunk_end = min(current_day + step_day - timedelta(seconds=1), end_utc_day)\n        yield current_day, chunk_end\n        current_day = current_day + step_day\n\n\n# --------------------------\n# OAuth2: refresh token\n# --------------------------\ndef refresh_access_token():\n\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    if not all([client_id, client_secret, refresh_token]):\n        raise ValueError(\"Faltan secrets: qb_client_id, qb_client_secret o qb_refresh_token\")\n\n    # Codificar client_id:client_secret en base64\n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode('utf-8')).decode('utf-8')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Authorization\": f\"Basic {encoded_credentials}\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, timeout=30)\n    try:\n        resp.raise_for_status()\n    except Exception as e:\n        raise RuntimeError(f\"Error al refrescar token: {resp.status_code} {resp.text}\") from e\n\n    tokens = resp.json()\n    new_access_token = tokens.get('access_token')\n    new_refresh_token = tokens.get('refresh_token')\n\n    if not new_access_token:\n        raise RuntimeError(\"Respuesta inv\u00e1lida: no se recibi\u00f3 access_token\")\n\n    # Rotaci\u00f3n de refresh_token\n    if new_refresh_token and new_refresh_token != refresh_token and set_secret_value:\n        try:\n            set_secret_value('qb_refresh_token', new_refresh_token)\n            print(\"Refresh token rotado en Mage Secrets.\")\n        except Exception:\n            print(\"No se pudo persistir el nuevo refresh_token en Mage Secrets. R\u00f3talo manualmente.\")\n\n    print(\"Access token refrescado exitosamente.\")\n    return new_access_token\n\n\n# --------------------------\n#Paginaci\u00f3n con l\u00f3gica de backoff implementada\n# --------------------------\ndef _qbo_query_page(session: requests.Session, base_url: str, realm_id: str,\n                    access_token: str, minor_version: int, qbo_sql: str,\n                    startposition: int, maxresults: int):\n    \"\"\"\n    Ejecuta UNA p\u00e1gina de la QBO SQL Query con manejo de rate limits (429) y 5xx (backoff + jitter).\n    \"\"\"\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere url base y minor_version\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain',\n    }\n    params = {\n        'query': f\"{qbo_sql} startposition {startposition} maxresults {maxresults}\",\n        'minorversion': minor_version,\n    }\n\n    max_retries = 6  #Numero m\u00e1ximo de reintentos\n    base_sleep = 1.5  # segundos para backofff exponencial\n\n    for attempt in range(1, max_retries + 1):\n        resp = session.get(url, headers=headers, params=params, timeout=60)\n        if resp.status_code == 429 or 500 <= resp.status_code < 600:  # 429(rate limit) , 5xx(Error del servidor)            \n            retry_after = resp.headers.get(\"Retry-After\") \n            if retry_after:  #Usamos valor dado por el retry after\n                sleep_s = float(retry_after)\n            else:\n                sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0, 0.3)  #Backoof exponencial con jitter aleatorio para evitar demasiados reintentos en el mismo tiempo \n            print(f\"Retry {attempt}/{max_retries} por {resp.status_code}. Durmiendo {sleep_s:.1f}s\")\n            time.sleep(sleep_s)\n            continue\n        try:\n            resp.raise_for_status()\n            data = resp.json() #Se devuelve la informacion en forma de diccionario\n            return data\n        except Exception as e:\n            raise RuntimeError(f\"Error QBO query (HTTP {resp.status_code}): {resp.text}\") from e \n\n    raise RuntimeError(\"Se agotaron los reintentos contra QBO.\")\n\n\n# --------------------------\n# Iterar todas las p\u00e1ginas\n# --------------------------\ndef _fetch_qb_data(session: requests.Session, realm_id: str, access_token: str,\n                   qbo_sql: str, base_url: str, minor_version: int, page_size: int = 1000):\n    \"\"\"\n    Itera todas las p\u00e1ginas hasta agotar resultados.\n    Devuelve data y m\u00e9tricas\n    \"\"\"\n    startposition = 1\n    total_rows = 0\n    page_number = 0\n    items = []\n\n    while True:\n        page_number += 1\n        t0 = time.time()\n        data = _qbo_query_page(\n            session, base_url, realm_id, access_token, minor_version,\n            qbo_sql, startposition, page_size\n        )\n        qr = data.get(\"QueryResponse\", {})\n        rows = qr.get(\"Invoice\", [])\n        items.extend(rows)\n\n        elapsed = time.time() - t0\n        print(f\"P\u00e1gina {page_number}: {len(rows)} filas en {elapsed:.2f}s (start={startposition})\")\n\n        total_rows += len(rows)\n        # Si trae menos que page_size, no hay m\u00e1s p\u00e1ginas\n        if len(rows) < page_size:\n            break\n\n        startposition += page_size\n        #Metricas solicitadas\n    metrics = {\n        \"pages\": page_number,\n        \"page_size\": page_size,\n        \"rows_total\": total_rows,\n    }\n    return items, metrics\n\n\n\n# --------------------------\n# Loader (block de Mage)\n# --------------------------\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Parametros a pasar:    \n      - fecha_inicio (\"2025-08-01T00:00:00Z\")\n      - fecha_fin    (\"2025-08-28T23:59:59Z\")\n      - days_per_chunk: int ( 1)\n      - page_size: int (1000)\n      - minor_version: int (75)\n    \"\"\"\n\n    realm_id = get_secret_value('qb_realm_id')\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n\n    if not all([realm_id, client_id, client_secret]):\n        raise ValueError(\"Faltan secrets: Realm_id, qb_client_id, qb_client_secret\")\n\n    access_token = refresh_access_token()\n\n    # Configuraci\u00f3n fechas\n    fecha_inicio = kwargs.get('fecha_inicio',\"2025-01-01\")\n    fecha_fin = kwargs.get('fecha_fin',\"2025-09-10\")\n    \n    \n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debe proporcionar 'fecha_inicio' y 'fecha_fin'.\")\n\n    start_utc = _parse_iso_utc(fecha_inicio)\n    end_utc = _parse_iso_utc(fecha_fin)\n\n    minor_version = int(kwargs.get('minor_version',25))\n    page_size = int(kwargs.get('page_size', 1000))\n    days_per_chunk = int(kwargs.get('days_per_chunk', 7))\n    \n    #Formaci\u00f3n del url\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    session = requests.Session()\n    all_rows = []\n    audit = []\n    chunk_idx = 0\n\n\n    for win_start, win_end in _date_range_chunks(start_utc, end_utc, days_per_chunk=days_per_chunk):\n        chunk_idx += 1\n        print(f\"Chunk {chunk_idx}: {win_start.isoformat()} \u2192 {win_end.isoformat()}\")\n        fecha_ini_str = win_start.strftime('%Y-%m-%d')\n        fecha_fin_str = win_end.strftime('%Y-%m-%d')\n\n        #SE CONSTRUYE CONSULTA PARA CADA RANGO DE FECHAS DEPENDIENDO DEL CHUNK\n        qbo_sql = f\"\"\"\n            select * from Invoice\n            where Metadata.CreateTime >= '{fecha_ini_str}' and Metadata.CreateTime <= '{fecha_fin_str}'\n            order by Metadata.CreateTime asc\n        \"\"\".strip()\n\n      \n\n        t0 = time.time()\n        rows, metrics = _fetch_qb_data(\n            session, realm_id, access_token, qbo_sql, base_url, minor_version, \n           \n            page_size=page_size\n        )\n        elapsed = time.time() - t0\n        for r in rows:\n            r[\"_win_start_utc\"] = win_start.isoformat()\n            r[\"_win_end_utc\"]   = win_end.isoformat()\n        all_rows.extend(rows)\n        \n        audit.append({\n            \"chunk\": chunk_idx,\n            \"window_start_utc\": win_start.isoformat(),\n            \"window_end_utc\": win_end.isoformat(),\n            \"pages\": metrics[\"pages\"],\n            \"page_size\": metrics[\"page_size\"],\n            \"rows\": metrics[\"rows_total\"],\n            \"duration_sec\": round(elapsed, 2),\n        })\n\n        \n\n    result = {\n        \"minor_version\": minor_version,\n        \"count\": len(all_rows),\n        \"data\": all_rows,\n        \"audit\": audit,\n        \"generated_at_utc\": _now_utc().isoformat(),\n    }\n\n    return result\n\n\n# --------------------------\n# Test del block\n# --------------------------\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output es None'\n    assert isinstance(output, dict), 'Output debe ser dict'\n    assert 'data' in output and isinstance(output['data'], list), 'Output[\"data\"] debe ser lista'\n    assert 'audit' in output and isinstance(output['audit'], list), 'Output[\"audit\"] debe ser lista'\n    if output.get('count', 0) > 0:\n        sample = output['data'][0]\n        assert isinstance(sample, dict), 'Cada fila debe ser un dict (JSON)'\n", "file_path": "data_loaders/qb_invoices_loader.py", "language": "python", "type": "data_loader", "uuid": "qb_invoices_loader"}, "data_loaders/qb_items_loader.py:data_loader:python:qb items loader": {"content": "import requests\nimport base64\nimport time\nimport random\nfrom datetime import datetime, timedelta, timezone\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\ntry:\n    from mage_ai.data_preparation.shared.secrets import set_secret_value\nexcept Exception:\n    set_secret_value = None\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n# --------------------------\n# Utilidades\n# --------------------------\ndef _now_utc():\n    return datetime.now(timezone.utc)\n\n\ndef _parse_iso_utc(s: str) -> datetime:\n    \"\"\"\n    Admite 'YYYY-MM-DD' (asume 00:00:00Z)\n    \"\"\"\n    if 'T' not in s:\n        s = s.strip() + \"T00:00:00Z\"\n    return datetime.fromisoformat(s.replace('Z', '+00:00')).astimezone(timezone.utc)\n\n\ndef _date_range_chunks(start_utc_day: datetime, end_utc_day: datetime, days_per_chunk: int = 1):\n    \"\"\"\nGenerador rango de ventanas chunk\n    \"\"\"\n    if start_utc_day.tzinfo is None or end_utc_day.tzinfo is None:\n        raise ValueError(\"Las fechas deben ser timezone-aware (UTC).\")\n    current_day = start_utc_day\n    step_day = timedelta(days=days_per_chunk)\n    while current_day <= end_utc_day:\n        chunk_end = min(current_day + step_day - timedelta(seconds=1), end_utc_day)\n        yield current_day, chunk_end\n        current_day = current_day + step_day\n\n\n\ndef refresh_access_token():\n\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    if not all([client_id, client_secret, refresh_token]):\n        raise ValueError(\"Faltan secrets: qb_client_id, qb_client_secret o qb_refresh_token\")\n\n    # Codificar client_id:client_secret en base64\n    credentials = f\"{client_id}:{client_secret}\"\n    encoded_credentials = base64.b64encode(credentials.encode('utf-8')).decode('utf-8')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Authorization\": f\"Basic {encoded_credentials}\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token,\n    }\n\n    resp = requests.post(url, headers=headers, data=data, timeout=30)\n    try:\n        resp.raise_for_status()\n    except Exception as e:\n        raise RuntimeError(f\"Error al refrescar token: {resp.status_code} {resp.text}\") from e\n\n    tokens = resp.json()\n    new_access_token = tokens.get('access_token')\n    new_refresh_token = tokens.get('refresh_token')\n\n    if not new_access_token:\n        raise RuntimeError(\"Respuesta inv\u00e1lida: no se recibi\u00f3 access_token\")\n\n    # Rotaci\u00f3n de refresh_token\n    if new_refresh_token and new_refresh_token != refresh_token and set_secret_value:\n        try:\n            set_secret_value('qb_refresh_token', new_refresh_token)\n            print(\"Refresh token rotado en Mage Secrets.\")\n        except Exception:\n            print(\"No se pudo persistir el nuevo refresh_token en Mage Secrets. R\u00f3talo manualmente.\")\n\n    print(\"Access token refrescado exitosamente.\")\n    return new_access_token\n\n\n# --------------------------\n#Paginaci\u00f3n con l\u00f3gica de backoff implementada\n# --------------------------\ndef _qbo_query_page(session: requests.Session, base_url: str, realm_id: str,\n                    access_token: str, minor_version: int, qbo_sql: str,\n                    startposition: int, maxresults: int):\n    \"\"\"\n    Ejecuta UNA p\u00e1gina de la QBO SQL Query con manejo de rate limits (429) y 5xx (backoff + jitter).\n    \"\"\"\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere url base y minor_version\")\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain',\n    }\n    params = {\n        'query': f\"{qbo_sql} startposition {startposition} maxresults {maxresults}\",\n        'minorversion': minor_version,\n    }\n\n    max_retries = 6  #Numero m\u00e1ximo de reintentos\n    base_sleep = 1.5  # segundos para backofff exponencial\n\n    for attempt in range(1, max_retries + 1):\n        resp = session.get(url, headers=headers, params=params, timeout=60)\n        if resp.status_code == 429 or 500 <= resp.status_code < 600:  # 429(rate limit) , 5xx(Error del servidor)            \n            retry_after = resp.headers.get(\"Retry-After\") \n            if retry_after:  #Usamos valor dado por el retry after\n                sleep_s = float(retry_after)\n            else:\n                sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0, 0.3)  #Backoof exponencial con jitter aleatorio para evitar demasiados reintentos en el mismo tiempo \n            print(f\"Retry {attempt}/{max_retries} por {resp.status_code}. Durmiendo {sleep_s:.1f}s\")\n            time.sleep(sleep_s)\n            continue\n        try:\n            resp.raise_for_status()\n            data = resp.json() #Se devuelve la informacion en forma de diccionario\n            return data\n        except Exception as e:\n            raise RuntimeError(f\"Error QBO query (HTTP {resp.status_code}): {resp.text}\") from e \n\n    raise RuntimeError(\"Se agotaron los reintentos contra QBO.\")\n\n\n# --------------------------\n# Iterar todas las p\u00e1ginas\n# --------------------------\ndef _fetch_qb_data(session: requests.Session, realm_id: str, access_token: str,\n                   qbo_sql: str, base_url: str, minor_version: int, page_size: int = 1000):\n    \"\"\"\n    Itera todas las p\u00e1ginas hasta agotar resultados.\n    Devuelve data y m\u00e9tricas\n    \"\"\"\n    startposition = 1\n    total_rows = 0\n    page_number = 0\n    items = []\n\n    while True:\n        page_number += 1\n        t0 = time.time()\n        data = _qbo_query_page(\n            session, base_url, realm_id, access_token, minor_version,\n            qbo_sql, startposition, page_size\n        )\n        qr = data.get(\"QueryResponse\", {})\n        rows = qr.get(\"Item\", [])\n        items.extend(rows)\n\n        elapsed = time.time() - t0\n        print(f\"P\u00e1gina {page_number}: {len(rows)} filas en {elapsed:.2f}s (start={startposition})\")\n\n        total_rows += len(rows)\n        # Si trae menos que page_size, no hay m\u00e1s p\u00e1ginas\n        if len(rows) < page_size:\n            break\n\n        startposition += page_size\n        #Metricas solicitadas\n    metrics = {\n        \"pages\": page_number,\n        \"page_size\": page_size,\n        \"rows_total\": total_rows,\n    }\n    return items, metrics\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Parametros a pasar:    \n      - fecha_inicio (\"2025-08-01T00:00:00Z\")\n      - fecha_fin    (\"2025-08-28T23:59:59Z\")\n      - days_per_chunk: int ( 1)\n      - page_size: int (1000)\n      - minor_version: int (75)\n    \"\"\"\n\n    realm_id = get_secret_value('qb_realm_id')\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n\n    if not all([realm_id, client_id, client_secret]):\n        raise ValueError(\"Faltan secrets: Realm_id, qb_client_id, qb_client_secret\")\n\n    access_token = refresh_access_token()\n\n    # Configuraci\u00f3n fechas\n    fecha_inicio = kwargs.get('fecha_inicio',\"2025-01-01\")\n    fecha_fin = kwargs.get('fecha_fin',\"2025-09-10\")\n    \n    \n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debe proporcionar 'fecha_inicio' y 'fecha_fin'.\")\n\n    start_utc = _parse_iso_utc(fecha_inicio)\n    end_utc = _parse_iso_utc(fecha_fin)\n\n    minor_version = int(kwargs.get('minor_version',25))\n    page_size = int(kwargs.get('page_size', 1000))\n    days_per_chunk = int(kwargs.get('days_per_chunk', 7))\n    \n    #Formaci\u00f3n del url\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    session = requests.Session()\n    all_rows = []\n    audit = []\n    chunk_idx = 0\n\n\n    for win_start, win_end in _date_range_chunks(start_utc, end_utc, days_per_chunk=days_per_chunk):\n        chunk_idx += 1\n        print(f\"Chunk {chunk_idx}: {win_start.isoformat()} \u2192 {win_end.isoformat()}\")\n        fecha_ini_str = win_start.strftime('%Y-%m-%d')\n        fecha_fin_str = win_end.strftime('%Y-%m-%d')\n\n        #SE CONSTRUYE CONSULTA PARA CADA RANGO DE FECHAS DEPENDIENDO DEL CHUNK\n        qbo_sql = f\"\"\"\n            select * from Item\n            where Metadata.CreateTime >= '{fecha_ini_str}' and Metadata.CreateTime <= '{fecha_fin_str}'\n            order by Metadata.CreateTime asc\n        \"\"\".strip()\n\n      \n\n        t0 = time.time()\n        rows, metrics = _fetch_qb_data(\n            session, realm_id, access_token, qbo_sql, base_url, minor_version, \n           \n            page_size=page_size\n        )\n        elapsed = time.time() - t0\n        for r in rows:\n            r[\"_win_start_utc\"] = win_start.isoformat()\n            r[\"_win_end_utc\"]   = win_end.isoformat()\n        all_rows.extend(rows)\n        \n        audit.append({\n            \"chunk\": chunk_idx,\n            \"window_start_utc\": win_start.isoformat(),\n            \"window_end_utc\": win_end.isoformat(),\n            \"pages\": metrics[\"pages\"],\n            \"page_size\": metrics[\"page_size\"],\n            \"rows\": metrics[\"rows_total\"],\n            \"duration_sec\": round(elapsed, 2),\n        })\n\n        \n\n    result = {\n        \"minor_version\": minor_version,\n        \"count\": len(all_rows),\n        \"data\": all_rows,\n        \"audit\": audit,\n        \"generated_at_utc\": _now_utc().isoformat(),\n    }\n\n    return result\n\n\n\n\n\n# --------------------------\n# Test del block\n# --------------------------\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output es None'\n    assert isinstance(output, dict), 'Output debe ser dict'\n    assert 'data' in output and isinstance(output['data'], list), 'Output[\"data\"] debe ser lista'\n    assert 'audit' in output and isinstance(output['audit'], list), 'Output[\"audit\"] debe ser lista'\n    if output.get('count', 0) > 0:\n        sample = output['data'][0]\n        assert isinstance(sample, dict), 'Cada fila debe ser un dict (JSON)'\n\n", "file_path": "data_loaders/qb_items_loader.py", "language": "python", "type": "data_loader", "uuid": "qb_items_loader"}, "pipelines/qb_customer_backfill/metadata.yaml:pipeline:yaml:qb customer backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - qb_customer_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: qb_customer_loader\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: qb_customer_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: qb_customer_exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - qb_customer_loader\n  uuid: qb_customer_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 21:12:09.423366+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customer_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customer_backfill\nvariables_dir: /home/src/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_customer_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customer_backfill/metadata"}, "pipelines/qb_customer_backfill/__init__.py:pipeline:python:qb customer backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customer_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customer_backfill/__init__"}, "pipelines/qb_invoices_bacfill/metadata.yaml:pipeline:yaml:qb invoices bacfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - qb_invoices_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: qb_invoices_loader\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: qb_invoices_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: qb_invoices_exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - qb_invoices_loader\n  uuid: qb_invoices_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 21:08:08.001159+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_bacfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_bacfill\nvariables_dir: /home/src/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_invoices_bacfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_bacfill/metadata"}, "pipelines/qb_invoices_bacfill/__init__.py:pipeline:python:qb invoices bacfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_bacfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_bacfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - qb_items_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: qb_items_loader\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: qb_items_loader\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: qb_items_exporter\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - qb_items_loader\n  uuid: qb_items_exporter\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-11 21:10:29.782176+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables_dir: /home/src/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}